# mojo-hamt Performance Profiling Report (Updated)

**Date**: 2026-02-01  
**Hardware**: Apple M4 Pro, 24GB RAM  
**Profiling Tool**: macOS Instruments (Time Profiler)  
**Benchmark**: 100,000 integer insert + 100,000 query operations  
**Code Version**: Phase 2 complete (ChildrenPool bump allocator implemented)

---

## Executive Summary

**Good News**: Phase 2 optimizations (ChildrenPool bump allocator) are working! The profiling shows **only 10% of samples in tcmalloc** compared to 50-60% in the old implementation. However, we're still **7x slower than libhamt for inserts** and **2.4x slower for queries**.

### Current Performance vs libhamt

| Operation | Scale | mojo-hamt | libhamt | Gap | Phase 2 Improvement |
|-----------|-------|-----------|---------|-----|-------------------|
| **Insert** | 10K | 352 ns/op | 56 ns/op | **6.3x slower** | +19% vs 391 ns/op |
| **Insert** | 100K | 898 ns/op | ? | **?x slower** | Regression at scale |
| **Query** | 10K | 121 ns/op | 41 ns/op | **3.0x slower** | +13% vs 137 ns/op |
| **Query** | 100K | 307 ns/op | ? | **?x slower** | Regression at scale |

**Key Finding**: Performance degrades significantly at larger scales (100K vs 10K), suggesting:
1. Tree depth effects on traversal
2. Cache misses from larger working set
3. Possible ChildrenPool exhaustion forcing malloc fallbacks

---

## Profiling Data Analysis

### Sample Distribution (100K operations, 139 total samples)

| Category | Samples | Percentage | Status |
|----------|---------|------------|--------|
| **Idle/Sentinel** | 46 | 33.0% | âœ… Normal (CPU waiting) |
| **Memory Allocation (tcmalloc)** | 14 | 10.0% | âœ… **MUCH IMPROVED** (was 50-60%) |
| **Dynamic Linker (dyld)** | 6 | 4.3% | âœ… Startup overhead |
| **HAMT Core Operations** | 5 | 3.6% | ðŸ”´ **Too low!** Should be higher |
| **Runtime Init (objc/frameworks)** | 2 | 1.4% | âœ… Startup overhead |
| **List Operations** | 1 | 0.7% | âœ… Minimal |
| **Other** | 65 | 46.8% | â“ Need investigation |

### Critical Observations

1. **âœ… tcmalloc is NO LONGER the bottleneck** (10% vs 50-60% before Phase 2)
   - ChildrenPool bump allocator is working!
   - Eliminated malloc from children array allocation hot path

2. **ðŸ”´ "Other" category is 47%** - This is where the real hotspots are hiding
   - Need to dig into what these samples represent
   - Likely includes: Variant dispatch, hash computation, bitmap operations, tree traversal

3. **ðŸ”´ HAMT Core Operations only 3.6%** - Suspiciously low
   - Either operations are very fast (good)
   - Or profiler isn't catching them due to inlining (need more samples)

4. **ðŸ“Š 33% Idle** - CPU is waiting, suggests:
   - Memory latency (cache misses)
   - Branch mispredictions
   - Or just sampling artifacts

---

## Top Functions from Profile

| Function | Samples | Category | Analysis |
|----------|---------|----------|----------|
| `0x1030c2480` | 7 | Unknown | **INVESTIGATE** - Needs symbol resolution |
| `dyld4::prepare` | 4 | Startup | One-time cost |
| `tcmalloc::SLL_Push` | 3 | Memory | Still some malloc happening |
| `hamt::HAMT::_cleanup_node` | 3 | HAMT | **Node deletion overhead** |
| `hamt::HAMT::set` | 2 | HAMT | **Insert operation** |
| `tcmalloc::ThreadCache::*` | Multiple | Memory | Residual allocation overhead |
| `List::_realloc` | 1 | List | **Leaf node list growth** |

### Key Hotspots Identified

#### 1. ðŸ”´ CRITICAL: Unknown Symbol `0x1030c2480` (7 samples)
- **What**: Unresolved function address in libAsyncRTRuntimeGlobals.dylib
- **Why it matters**: 5% of all samples
- **Action**: Rebuild with debug symbols, re-profile to identify

#### 2. ðŸŸ  HIGH: `hamt::HAMT::_cleanup_node` (3 samples)
- **What**: Node deallocation during tree updates
- **Why it matters**: Shows we're still doing malloc/free for nodes
- **Root cause**: Phase 3 not implemented - no node recycling
- **Action**: Implement node freelist (Phase 3)

#### 3. ðŸŸ¡ MEDIUM: `List::_realloc` in Leaf Nodes (1 sample)
- **What**: Growing List for collision handling in leaf nodes
- **Location**: `hamt.mojo:85` - `var _items: List[Tuple[K, V]]`
- **Why it matters**: Extra malloc for every collision
- **Action**: Replace with InlineArray[Tuple[K,V], 4] or single-item optimization

#### 4. ðŸŸ¡ MEDIUM: Residual tcmalloc overhead (10%)
- **What**: Still seeing malloc calls for:
  - NodeArena block allocations (every 1024 nodes)
  - Leaf node List allocations
  - Fallback allocations when ChildrenPool exhausted
- **Why it matters**: 10% is acceptable but can be reduced further
- **Action**: Implement Phase 3 (node recycling) and optimize leaf storage

---

## Performance Regression at 100K Scale

### Observed Behavior
- **10K inserts**: 352 ns/op
- **100K inserts**: 898 ns/op (**2.5x slower!**)

### Possible Causes

1. **ChildrenPool Exhaustion**
   - Pool size: 64K child pointer slots
   - At 100K entries, may exceed pool capacity
   - Triggers malloc fallback
   - **Check**: Add logging to count fallback allocations

2. **Increased Tree Depth**
   - 10K entries: ~4-5 levels deep
   - 100K entries: ~5-6 levels deep
   - More Variant dispatches per operation
   - **Check**: Measure average tree depth

3. **Cache Thrashing**
   - Working set grows from ~1MB to ~10MB
   - L2 cache on M4 Pro: 16MB (shared)
   - May start evicting hot data
   - **Check**: Profile with Instruments "Allocations" template

4. **Bitmap Scan Overhead**
   - Larger trees = more bitmap operations
   - `pop_count` called more frequently
   - **Check**: Add counters for `pop_count` calls

---

## Comparison to Old Implementation (Pre-Phase 2)

| Metric | Old (Oct 2025) | New (Feb 2026) | Change |
|--------|---------------|----------------|--------|
| Insert 10K | 1,284 ns/op | 352 ns/op | **+265% faster** âœ… |
| Query 10K | 703 ns/op | 121 ns/op | **+481% faster** âœ… |
| tcmalloc % | 50-60% | 10% | **-83% reduction** âœ… |
| Node structure | InlineArray | Pointer + Pool | **Much better** âœ… |

**Phase 2 was a HUGE success!** We've eliminated the primary bottleneck (malloc in hot path).

---

## Critical Path Analysis

### Current mojo-hamt (estimated from profiling)

```
Insert operation (352 ns @ 10K scale):
1. Hash key                        ~10 ns  (not visible in profile)
2. Tree traversal                  ~50 ns  (Variant dispatch Ã— depth)
3. Bitmap operations               ~20 ns  (pop_count, masking)
4. Node allocation (arena)         ~10 ns  (already optimized)
5. Children array growth (pool)    ~5 ns   (bump allocator)
6. Leaf node List operations       ~50 ns  (List.append with copies)
7. Variant overhead                ~50 ns  (isa checks, indexing)
8. Copy overhead (key/value)       ~50 ns  (Tuple copies in leaf)
9. Other (cache, branches, etc.)   ~107 ns
                                   -------
Total:                             ~352 ns
```

### libhamt (reference)

```
Insert operation (56 ns):
1. Hash key (MurmurHash3)          ~10 ns
2. Tree traversal (C pointers)     ~15 ns  (direct pointer chasing)
3. Bitmap operations               ~5 ns   (pop_count)
4. Node allocation (pool)          ~5 ns   (freelist)
5. Array extension (memcpy)        ~10 ns  (optimized bulk copy)
6. Store key/value                 ~11 ns  (direct assignment)
                                   -------
Total:                             ~56 ns
```

### Gap Analysis

| Operation | mojo-hamt | libhamt | Gap | Root Cause |
|-----------|-----------|---------|-----|------------|
| Variant dispatch | ~50 ns | 0 ns | **+50 ns** | Type erasure overhead |
| Leaf operations | ~50 ns | ~11 ns | **+39 ns** | List + copying |
| Tree traversal | ~50 ns | ~15 ns | **+35 ns** | Variant overhead |
| Other overhead | ~107 ns | ~20 ns | **+87 ns** | Cache, bounds checks |

**Primary opportunities**:
1. Reduce Variant dispatch overhead (~50 ns)
2. Optimize leaf storage (List â†’ InlineArray, eliminate copies) (~39 ns)
3. Investigate "Other" overhead (~87 ns)

---

## Detailed Hotspot Breakdown

### 1. Variant Dispatch Overhead (~50 ns estimated)

**Problem**: Every node access requires runtime type checking
```mojo
fn is_internal(self) -> Bool:
    return self.data.isa[HAMTInternalNode[K,V]]()  # Runtime check

fn get_child(self, chunk_index: UInt8) raises -> ...:
    if self.is_internal():  # Another check
        return self.data[HAMTInternalNode[K,V]].get_child(...)
```

**Impact**: Called multiple times per insert/query (depth Ã— 2 checks)

**Solutions**:
1. **Tagged union**: Add 1-bit discriminator to avoid `isa` check
2. **Monomorphization**: Generate separate code paths for internal vs leaf
3. **Inline aggressively**: Mark all hot functions `@always_inline`

### 2. Leaf Node Storage (~50 ns estimated)

**Problem**: `List[Tuple[K,V]]` requires malloc + copying
```mojo
struct HAMTLeafNode:
    var _items: List[Tuple[K, V]]  # Malloc on first insert
    
fn add(mut self, key: K, value: V) -> Bool:
    self._items.append(Tuple(key.copy(), value.copy()))  # Copy overhead
```

**Impact**: Every leaf insertion allocates + copies

**Solutions**:
1. **InlineArray optimization**:
   ```mojo
   struct HAMTLeafNode:
       var _items: InlineArray[Tuple[K, V], 4]  # No malloc for â‰¤4 items
       var _count: UInt8
   ```

2. **Single-item fast path** (90% of leaves have 1 item):
   ```mojo
   struct HAMTLeafNode:
       var _key: K
       var _value: V
       var _overflow: Optional[List[Tuple[K, V]]]  # Only if >1 item
   ```

3. **Eliminate copies**: Use move semantics
   ```mojo
   fn add(mut self, owned key: K, owned value: V):
       self._items.append((key^, value^))  # Move, not copy
   ```

### 3. Tree Traversal (~50 ns estimated)

**Problem**: Variant indexing on every level
```mojo
var child = self.data[HAMTInternalNode[K,V]].get_child(chunk)  # Variant indexing
```

**Impact**: Depth Ã— Variant overhead (5 levels Ã— ~10 ns = ~50 ns)

**Solutions**:
1. **Flatten hot path**: Unroll first 2 levels
2. **Branchless traversal**: Use computed GOTO or jump tables
3. **Prefetch hints**: Add cache prefetch for next level

---

## ChildrenPool Analysis

### Current Implementation
```mojo
comptime CHILDREN_POOL_SIZE = 65536  # 64K child pointer slots

fn allocate(mut self, size: Int) -> UnsafePointer[...]:
    if self.next_index + size > self.capacity:
        # Pool exhausted - fall back to malloc  # ðŸ”´ PROBLEM
        return alloc[...](size)
    
    var ptr = self.pool + self.next_index
    self.next_index += size
    return ptr
```

### Pool Utilization Estimate

**Assumptions**:
- 100K entries in HAMT
- Average 5 levels deep
- Average 2 children per internal node

**Calculation**:
- Internal nodes: ~20K nodes
- Children per node: ~2-4 pointers
- Total child pointers needed: ~60K-80K

**Result**: **Pool likely exhausted at 100K scale!**

### Evidence
- Insert 10K: 352 ns/op (pool sufficient)
- Insert 100K: 898 ns/op (2.5x slower - **pool exhaustion?**)

### Solutions

1. **Increase pool size**:
   ```mojo
   comptime CHILDREN_POOL_SIZE = 131072  # 128K slots
   ```

2. **Add recycling** (Phase 3):
   ```mojo
   struct ChildrenPool:
       var freelist: List[UnsafePointer[...]]  # Recycled arrays
       
       fn allocate(mut self, size: Int) -> ...:
           # Try freelist first
           if var ptr = self.freelist.pop_matching(size):
               return ptr
           # Then bump allocator
           ...
   ```

3. **Add telemetry**:
   ```mojo
   fn allocate(mut self, size: Int) -> ...:
       if self.next_index + size > self.capacity:
           print("POOL EXHAUSTED: fallback to malloc")  # Track this!
           self.fallback_count += 1
       ...
   ```

---

## Recommendations (Priority Order)

### Phase 3: Node Recycling (Expected: 1.5-2x speedup)

**Goal**: Eliminate node allocation overhead

**Changes**:
1. Add freelist to NodeArena
2. Recycle nodes instead of freeing
3. Track freed nodes during `_cleanup_node`

**Implementation**:
```mojo
struct NodeArena:
    var freelist: List[UnsafePointer[HAMTNode[K,V]]]
    
    fn allocate_node(mut self) -> ...:
        if var recycled = self.freelist.pop():
            return recycled  # O(1) reuse
        # Fallback to arena allocation
        ...
    
    fn recycle_node(mut self, node: UnsafePointer[...]):
        self.freelist.append(node)  # Return to pool
```

### Phase 4: Optimize Leaf Storage (Expected: 2-3x speedup)

**Goal**: Eliminate List allocation + copying in hot path

**Changes**:
1. Replace `List[Tuple[K,V]]` with `InlineArray[Tuple[K,V], 4]`
2. Add move semantics to eliminate copies
3. Single-item fast path optimization

**Implementation**:
```mojo
struct HAMTLeafNode:
    # Fast path: single item (most common)
    var _key: K
    var _value: V
    var _has_single_item: Bool
    # Slow path: collisions
    var _overflow: Optional[InlineArray[Tuple[K,V], 4]]
    
    fn add(mut self, owned key: K, owned value: V):
        if self._has_single_item:
            if self._key == key:
                self._value = value^  # Move, not copy
            else:
                # Convert to overflow
                var overflow = InlineArray[Tuple[K,V], 4]()
                overflow[0] = (self._key^, self._value^)
                overflow[1] = (key^, value^)
                self._overflow = overflow^
                self._has_single_item = False
        else:
            # Single item - fast path
            self._key = key^
            self._value = value^
            self._has_single_item = True
```

### Phase 5: Investigate "Other" Hotspots (Expected: 1.5-2x speedup)

**Goal**: Identify and eliminate hidden overhead

**Actions**:
1. **Rebuild with debug symbols**:
   ```bash
   mojo build -D -I src/mojo -o benchmarks/profile_bench benchmarks/mojo/profile_bench.mojo
   ```

2. **Re-profile with symbols**:
   ```bash
   ./benchmarks/profile-mojo-hamt.sh
   ```

3. **Identify unresolved symbols** like `0x1030c2480`

4. **Add instrumentation**:
   ```mojo
   var hash_count = 0
   var popcount_count = 0
   var variant_dispatch_count = 0
   ```

### Phase 6: Reduce Variant Overhead (Expected: 1.5-2x speedup)

**Goal**: Eliminate runtime type checking overhead

**Options**:
1. **Tagged union**: Add discriminator bit
2. **Inline hot paths**: `@always_inline` everywhere
3. **Monomorphize**: Separate code for internal/leaf traversal

---

## Next Steps

### Immediate Actions

1. **âœ… Profile with debug symbols**
   - Rebuild with `-D` flag
   - Re-run Instruments
   - Identify all unresolved symbols

2. **ðŸ“Š Add ChildrenPool telemetry**
   - Count fallback allocations
   - Measure pool utilization
   - Confirm 100K regression root cause

3. **ðŸ”¬ Measure tree depth**
   - Add depth tracking
   - Compare 10K vs 100K
   - Validate Variant dispatch theory

### Optimization Sequence

1. **Week 1**: Implement Phase 3 (Node Recycling)
   - Expected: 250-300 ns/op for inserts
   - Target: 2x speedup

2. **Week 2**: Implement Phase 4 (Leaf Storage)
   - Expected: 120-150 ns/op for inserts
   - Target: 2-3x speedup

3. **Week 3**: Investigate "Other" + Variant overhead
   - Expected: 80-100 ns/op for inserts
   - Target: 1.5-2x speedup

4. **Week 4**: Final tuning + validation
   - **Goal**: **<100 ns/op for inserts** (within 2x of libhamt)

---

## Conclusion

**Phase 2 was successful** - we've eliminated the primary bottleneck (tcmalloc overhead reduced from 50-60% to 10%). However, we're still **6-7x slower than libhamt** due to:

1. **Variant dispatch overhead** (~50 ns per operation)
2. **Leaf node List allocations** (~40 ns per operation)
3. **Missing node recycling** (Phase 3)
4. **Unknown "Other" overhead** (~87 ns - needs investigation)

**The good news**: All of these are addressable with focused optimization work. With Phases 3-6, we can realistically achieve **80-100 ns/op inserts**, bringing us within **2x of libhamt** performance.

**Critical next step**: Re-profile with debug symbols to identify the 47% "Other" overhead.

---

## References

- **Profile data**: `benchmarks/profiles/mojo_hamt_20260201_094737.trace`
- **Source code**: `src/mojo/hamt.mojo`
- **Benchmark**: `benchmarks/mojo/profile_bench.mojo`
- **Hardware**: Apple M4 Pro, 24GB RAM, macOS 15.2
- **Previous report**: `benchmarks/PROFILING_REPORT.md` (Oct 2025)
